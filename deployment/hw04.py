# -*- coding: utf-8 -*-
"""hw04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NkP9rcXIs_LL2wEWSPKqTIDDpkO1c2Iz
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, auc
from typing import List, Tuple, Dict
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold

# !wget https://archive.ics.uci.edu/static/public/222/bank+marketing.zip
# !unzip bank+marketing.zip
# !unzip bank.zip

df = pd.read_csv('/content/bank-full.csv', sep=';')
df.head(3)

df.y.value_counts()

columns = [
    'age', 'job', 'marital', 'education', 'balance', 'housing',
    'contact', 'day', 'month', 'duration', 'campaign', 'pdays',
    'previous', 'poutcome', 'y'
]
df = df[columns]

df.shape

df.head(3)

# binary_labels = {'no': 0, 'yes': 1}

#df['y'] = df['y'].map(binary_labels)

df.y = (df.y == 'yes').astype(int)

df.y.value_counts()

df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)
df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)

df_train = df_train.reset_index(drop=True)
df_val = df_val.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)

y_train = df_train.y.values
y_val = df_val.y.values
y_test = df_test.y.values

del df_train['y']
del df_val['y']
del df_test['y']

print(df_train.shape)
print(df_val.shape)
print(df_test.shape)

df_train.head()

"""**Question 1: ROC AUC feature importance**

ROC AUC could also be used to evaluate feature importance of numerical variables.

Let's do that

For each numerical variable, use it as score and compute AUC with the y variable
Use the training dataset for that
If your AUC is < 0.5, invert this variable by putting "-" in front

(e.g. -df_train['engine_hp'])

AUC can go below 0.5 if the variable is negatively correlated with the target variable. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.

Which numerical variable (among the following 4) has the highest AUC?


* balance
* day
* duration
* previous
"""

numerical_data = df_train.select_dtypes(include=['number']).columns.tolist()
numerical_data

categorical_data = df_train.select_dtypes(include=['object', 'category']).columns.tolist()
categorical_data

numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

categorical_columns = ['job', 'marital', 'education', 'housing', 'contact', 'month', 'poutcome']

df['y'].value_counts()

def find_roc_auc_score(columns: List[int]) -> Tuple[dict, str]:
  auc_scores = {}
  for col in numerical_columns:
    auc = roc_auc_score(df['y'], df[col])
    # If AUC < 0.5, invert the variable
    if auc < 0.5:
        auc = roc_auc_score(df['y'], -df[col])
    auc_scores[col] = auc
  # Find the numerical variable with the highest AUC
  best_feature = max(auc_scores, key=auc_scores.get)
  return auc_scores, best_feature

auc_scores, best_feature = find_roc_auc_score(numerical_columns)
# Output the AUC scores and the best feature
print("AUC Scores:", auc_scores)
print("Best numerical feature by AUC:", best_feature)

"""**Question 2:**

Training the model
Apply one-hot-encoding using DictVectorizer and train the logistic regression with these parameters:

*LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)*
"""

columns = numerical_columns + categorical_columns
train_dicts = df_train[columns].to_dict(orient='records')
dv = DictVectorizer(sparse=False)
X_train = dv.fit_transform(train_dicts)

model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)
model.fit(X_train, y_train)

val_dicts = df_val[columns].to_dict(orient='records')
X_val = dv.transform(val_dicts)

y_pred = model.predict_proba(X_val)[:, 1]

roc_auc_score(y_val, y_pred)

# Assuming you've already split your data and have y_val
y_scores = model.predict_proba(X_val)[:, 1]  # Probability of the positive class

# Calculate the ROC curve using the validation set
fpr, tpr, thresholds = roc_curve(y_val, y_scores)

# Calculate the AUC
roc_auc = auc(fpr, tpr)

# Output the AUC score
print("AUC:", roc_auc)

y_pred_bin = model.predict(X_val)

"""**Question 3:**

 Precision and Recall
"""

fpr[:10]

plt.figure(figsize=(5, 5))

# ROC curve for predicted probabilities
fpr, tpr, _ = roc_curve(y_val, y_pred)
plt.plot(fpr, tpr, label='Probability')

# ROC curve for binary (hard) predictions
fpr, tpr, _ = roc_curve(y_val, y_pred_bin)
plt.plot(fpr, tpr, label='Hard Prediction')

# Diagonal line representing random guessing
plt.plot([0, 1], [0, 1], color='grey', linestyle='--')

# Set the title and axis labels
plt.title('ROC Curve for Probability and Hard Predictions')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')

# Add legend and show the plot
plt.legend()
plt.show()

from sklearn.metrics import roc_curve

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_val, y_pred)

# Calculate Youden's J statistic for each threshold
youden_j = tpr - fpr
print(youden_j[:10])

idx = 0
max_value = 0

for i in range(len(youden_j)):
  if youden_j[i] > max_value:
    max_value = youden_j[i]
    idx = i
print(idx)
print(max_value)

# Find the index of the maximum Youden's J statistic
best_index = youden_j.argmax()
#print(best_index)
# Find the best threshold
best_threshold = thresholds[best_index]
#print(best_threshold)
# Plot the ROC curve
plt.figure(figsize=(5, 5))

plt.plot(fpr, tpr, label='Probability')

# Diagonal line representing random guessing
plt.plot([0, 1], [0, 1], color='grey', linestyle='--')

# Highlight the best threshold point
plt.scatter(fpr[best_index], tpr[best_index], color='red', label=f'Best Threshold = {best_threshold:.2f}', zorder=10)

# Set the title and axis labels
plt.title('ROC Curve with Best Threshold')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')

# Add legend and show the plot
plt.legend()
plt.show()

print(f'The best threshold is: {best_threshold:.2f}')

print(y_val[:15])
print(y_pred[:15].round(3))

def confusion_matrix_dataframe(y_val, y_pred):
    scores = []

    thresholds = np.linspace(0, 1, 101)

    for t in thresholds:
        actual_positive = (y_val == 1)
        actual_negative = (y_val == 0)

        predict_positive = (y_pred >= t)
        predict_negative = (y_pred < t)

        tp = (predict_positive & actual_positive).sum()
        tn = (predict_negative & actual_negative).sum()

        fp = (predict_positive & actual_negative).sum()
        fn = (predict_negative & actual_positive).sum()

        scores.append((t, tp, fp, fn, tn))

    columns = ['threshold', 'tp', 'fp', 'fn', 'tn']
    df_scores = pd.DataFrame(scores, columns=columns)

    return df_scores


df_scores = confusion_matrix_dataframe(y_val, y_pred)
df_scores[::10]

df_scores['p'] = df_scores.tp / (df_scores.tp + df_scores.fp)
df_scores['r'] = df_scores.tp / (df_scores.tp + df_scores.fn)

df_scores[::10]

import matplotlib.pyplot as plt

plt.plot(df_scores.threshold, df_scores.p, label='precision')
plt.plot(df_scores.threshold, df_scores.r, label='recall')

# Set x and y labels
plt.xlabel('Threshold')
plt.ylabel('Precision/Recall')

plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

df_scores[df_scores['p'].round(1) == df_scores['r'].round(1)]

"""**Question 4:**

F1 score
Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both

This is the formula for computing F1:

F
1
=
2
⋅
P
⋅
R
P
+
R

Where
P
 is precision and
R
 is recall.

Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01

At which threshold F1 is maximal?

0.02
0.22
0.42
0.62
"""

df_scores['f1'] = 2 * (df_scores['p'] * df_scores['r']) / (df_scores['p'] + df_scores['r'])

df_scores[::10]

best_f1 = df_scores[df_scores['f1'] == df_scores['f1'].max()]
best_f1

plt.plot(df_scores['threshold'], df_scores['f1'])
plt.scatter(best_f1['threshold'], best_f1['f1'], color='red', label=f"Best Threshold = {best_f1['threshold']}:.2f", zorder=10)

plt.xlabel('Threshold')

plt.ylabel('F1')
plt.title('F1-Threshold curve')
plt.legend()
plt.grid()
plt.show()

"""**Question 5:**

*5-Fold CV*

Use the KFold class from Scikit-Learn to evaluate our model on 5 different folds:

*KFold(n_splits=5, shuffle=True, random_state=1)*
Iterate over different folds of df_full_train
Split the data into train and validation
Train the model on train with these parameters: *LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)*
Use AUC to evaluate the model on validation
How large is standard deviation of the scores across different folds?

0.0001
0.006
0.06
0.26
"""

def train_model(df_train, y_train, C=1.0):
  dicts = df_train[columns].to_dict(orient='records')
  dv = DictVectorizer(sparse=False)

  X_train = dv.fit_transform(dicts)
  model = LogisticRegression(solver='liblinear', C=C)
  model.fit(X_train, y_train)
  return dv, model

def predict(df, dv, model):
  dicts = df[columns].to_dict(orient='records')
  X_val = dv.transform(dicts)
  y_pred = model.predict_proba(X_val)[:, 1]
  return y_pred

def cross_validation(df_full_train):

  auc_scores = []

  kfold = KFold(n_splits=5, shuffle=True, random_state=1)

  for train_idx, val_idx in kfold.split(df_full_train):
    df_train = df_full_train.iloc[train_idx]
    df_val = df_full_train.iloc[val_idx]

    y_train = df_train.y
    y_val = df_val.y

    dv, model = train_model(df_train, y_train)
    y_pred = predict(df_val, dv, model)

    auc = roc_auc_score(y_val, y_pred)
    auc_scores.append(auc)
  return auc_scores

result = cross_validation(df_full_train)
print(f'Mean AUC score: {np.mean(result)}')
print(f'STD: +- {np.std(result).round(3)}')

"""**Question 6:**

Hyperparameter Tuning

Now let's use 5-Fold cross-validation to find the best parameter C

Iterate over the following C values: [0.000001, 0.001, 1]
Initialize KFold with the same parameters as previously
Use these parameters for the model: *LogisticRegression(solver='liblinear', C=C, max_iter=1000)*
Compute the mean score as well as the std (round the mean and std to 3 decimal digits)
Which C leads to the best mean score?

0.000001
0.001
1

If you have ties, select the score with the lowest std. If you still have ties, select the smallest C.
"""

def find_best_c(df_full_train: pd.DataFrame) -> List[float]:
  kfold = KFold(n_splits=5, shuffle=True, random_state=1)
  results = {}
  mean_results = {}
  std_results = {}
  for C in [0.000001, 0.001, 1]:
      auc_scores = []
      for train_idx, val_idx in kfold.split(df_full_train):
          df_train = df_full_train.iloc[train_idx]
          df_val = df_full_train.iloc[val_idx]

          y_train = df_train['y']
          y_val = df_val['y']

          dv, model = train_model(df_train, y_train, C=C)
          y_pred = predict(df_val, dv, model)

          auc = roc_auc_score(y_val, y_pred)
          auc_scores.append(auc.round(3))
      results[f'C_{C}'] = auc_scores
      mean_results[f'C_{C}'] = np.mean(auc_scores).round(3)
      std_results[f'C_{C}'] = np.std(auc_scores).round(3)
  return results, mean_results, std_results

result, mean_auc, std_auc_scores = find_best_c(df_full_train)
print(result)
#mean_c_list = {k: np.mean(v) for k, v in c_list.items()}
print(mean_auc)
print(std_auc_scores)

# Create a DataFrame for fold-wise AUC scores
df_auc = pd.DataFrame(result)

# Alternatively, you can directly assign mean and std values from the dictionaries
df_auc.loc['Mean'] = mean_auc
df_auc.loc['Std'] = std_auc_scores

df_auc

df_train.head()

y_train[:10]

dv, model = train_model(df_train, y_train, C=1.0)

print(dv, model)

import pickle

C = 1.0

output_file = f'model_C={C}.bin'
output_file

f_out = open(output_file, 'wb')
pickle.dump((dv, model), f_out)
f_out.close()

# with open(output_file, 'wb') as f_out:
#   pickle.dump((dv, model), f_out)

with open(output_file, 'rb') as f_in:
  data = pickle.load(f_in)

data

customer = df_train.loc[0]
customer.to_dict()

dv_pickle, model_pickle = data

dv_pickle, model_pickle

X = dv.transform([customer])
X

X_pickle = dv_pickle.transform([customer])
X

res = model.predict_proba(X_pickle)[0, 1]
res